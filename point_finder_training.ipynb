{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE BROKE SOMETHING IN THE TRAINING OF THE POINT FINDER.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import torch\n",
    "import utils\n",
    "import config\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import resnet34\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import normalize_brightness, histogram_eq_global\n",
    "from scipy.fft import fft2, ifft2, fftshift\n",
    "import cv2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "size = 256\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_image(img_path, radius):\n",
    "    # Check if the image file exists\n",
    "    if not os.path.exists(img_path):\n",
    "        raise FileNotFoundError(f\"Image file not found: {img_path}\")\n",
    "\n",
    "    # Load and convert image to double precision (range [0, 1])\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Unable to read the image file: {img_path}\")\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0\n",
    "\n",
    "    # Apply histogram equalization\n",
    "    img_eq = histogram_eq_global((img_rgb * 255).astype(np.uint8)) / 255.0\n",
    "\n",
    "    # Perform FFT on the V channel of the equalized image\n",
    "    v_channel = img_eq[:, :, 2]\n",
    "    img_f = fft2(v_channel)\n",
    "\n",
    "    # Create circular mask in the frequency domain\n",
    "    height, width = v_channel.shape\n",
    "    y, x = np.ogrid[:height, :width]\n",
    "    center_y, center_x = height // 2, width // 2\n",
    "    mask = np.ones((height, width), dtype=np.float32)\n",
    "    mask[(y - center_y)**2 + (x - center_x)**2 < radius**2] = 0\n",
    "\n",
    "    # Apply the mask in the frequency domain\n",
    "    img_f_filtered = img_f * fftshift(mask)\n",
    "\n",
    "    # Perform inverse FFT to get the filtered image\n",
    "    f_ed = np.abs(ifft2(img_f_filtered))\n",
    "\n",
    "    # Resize the original RGB image and the filtered image to (256, 256)\n",
    "    img_rgb_resized = cv2.resize(img_rgb, (256, 256))\n",
    "    f_ed_resized = cv2.resize(f_ed, (256, 256))\n",
    "\n",
    "    # Stack the RGB image and the filtered image into a tensor\n",
    "    combined_image = np.concatenate([img_rgb_resized, f_ed_resized[:, :, np.newaxis]], axis=-1)\n",
    "\n",
    "    # Convert to a PyTorch tensor\n",
    "    combined_tensor = torch.tensor(combined_image, dtype=torch.float32)\n",
    "\n",
    "    return combined_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class StevenCustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.data = self.data.dropna(subset=[\"kp-1\"])\n",
    "        self.imgSize = size\n",
    "        self.pointImages = torch.empty((len(self.data), 3, self.imgSize, self.imgSize)).to(device)\n",
    "        self.images = torch.empty((len(self.data), 4, self.imgSize, self.imgSize)).to(device)\n",
    "\n",
    "        for idx in tqdm(range(len(self.data)), desc=\"Processing items\"):\n",
    "            img_path = config.DATASET_PATH + utils.get_img_path(self.data.iloc[idx][\"image\"])\n",
    "            \n",
    "            coordinates = self.__get_coordinates__(idx)\n",
    "\n",
    "            self.images[idx] = filter_image(img_path, 20).permute(2, 0, 1).float().to(device)\n",
    "\n",
    "            \n",
    "\n",
    "            # Convert coordinates and labels to tensors\n",
    "            coordinates = torch.tensor(coordinates, dtype=torch.float32)\n",
    "            labels = coordinates[:, 2].long()  # Labels for each keypoint\n",
    "            \n",
    "            x_scaled = coordinates[:, 0] / 100 * self.imgSize  # X positions scaled to image size\n",
    "            y_scaled = coordinates[:, 1] / 100 * self.imgSize  # Y positions scaled to image size\n",
    "\n",
    "            # Create a grid of pixel coordinates (image_size x image_size)\n",
    "            x_grid, y_grid = torch.meshgrid(torch.arange(self.imgSize), torch.arange(self.imgSize), indexing='ij')\n",
    "            x_grid, y_grid = x_grid.float(), y_grid.float()\n",
    "\n",
    "            # Calculate distances to each coordinate point in a vectorized manner\n",
    "            distances = torch.sqrt((x_grid.unsqueeze(0) - x_scaled.view(-1, 1, 1)) ** 2 + \n",
    "                                   (y_grid.unsqueeze(0) - y_scaled.view(-1, 1, 1)) ** 2)\n",
    "\n",
    "            # Find the closest coordinate for each pixel\n",
    "            closest_distances, closest_indices = torch.min(distances, dim=0)\n",
    "            closest_labels = labels[closest_indices]\n",
    "\n",
    "            # Apply Gaussian function to closest distances\n",
    "            gaussian_values = torch.clamp(self.gaussian(x=0.0+closest_distances/255, sig=0.02), max=1.0)\n",
    "\n",
    "            # Assign values to the pointImages tensor at the specific channel for each label\n",
    "            self.pointImages[idx, 0, torch.arange(self.imgSize), torch.arange(self.imgSize).view(-1, 1)] = gaussian_values.to(device)\n",
    "            #self.pointImages[idx, 1, torch.arange(self.imgSize), torch.arange(self.imgSize).view(-1, 1)] = gaussian_values.to(device)\n",
    "            #self.pointImages[idx, 2, torch.arange(self.imgSize), torch.arange(self.imgSize).view(-1, 1)] = gaussian_values.to(device)\n",
    "\n",
    "            #self.pointImages[idx, 2, :, :] = gaussian_values.to(device).permute(1, 0)\n",
    "\n",
    "            #print(\"max\",max)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def gaussian(self, x, mu=0, sig=0.02):\n",
    "        return torch.exp(-((x - mu) ** 2) / (2 * (sig ** 2)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        pointImg = self.pointImages[idx]\n",
    "        #max = torch.max(pointImg)\n",
    "        #print(\"max at getItem\",max)\n",
    "        return img, pointImg\n",
    "\n",
    "    def __get_label__(self, idx):\n",
    "        img_metadata_str = str(self.data.iloc[idx].get(\"kp-1\", \"nan\"))\n",
    "        img_metadata = json.loads(img_metadata_str)\n",
    "\n",
    "        contact_labels = [\n",
    "            metadata.get(\"keypointlabels\") == [\"Contact\"]\n",
    "            for metadata in img_metadata if isinstance(metadata, dict)\n",
    "        ]\n",
    "        return int(all(contact_labels))\n",
    "\n",
    "    def __get_coordinates__(self, idx):\n",
    "        img_metadata_str = self.data.iloc[idx][\"kp-1\"]\n",
    "        img_metadata = json.loads(img_metadata_str) if img_metadata_str else []\n",
    "\n",
    "        if len(img_metadata) == 0:\n",
    "            print(f\"No metadata found for index {idx}\")\n",
    "            return []\n",
    "\n",
    "        coordinates = [\n",
    "            [metadata.get('x'), metadata.get('y'), 0 if metadata.get('keypointlabels') == ['Contact'] else 1]\n",
    "            for metadata in img_metadata if isinstance(metadata, dict)\n",
    "        ]\n",
    "        return coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class features_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #The initial convolutional layer\n",
    "        self.preparation_for_resnet = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 4, out_channels = 3, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1)\n",
    "        ) \n",
    "        \n",
    "        #The resnet part of the network\n",
    "        self.resnet = resnet34()\n",
    "        self.resnet = nn.Sequential(*(list(self.resnet.children())[:-3]))\n",
    "\n",
    "        #The \"upsampling\" part of the network\n",
    "        self.geo_net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.UpsamplingBilinear2d(scale_factor=4),\n",
    "            nn.Conv2d(in_channels=8, out_channels=3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(3, out_channels=3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "        )\n",
    "    \n",
    "    '''  '''  \n",
    "    \n",
    "    #img is 3x64x64 and depth is 1x64x64\n",
    "    def forward(self, img):\n",
    "        x = self.preparation_for_resnet(img)\n",
    "        x = self.resnet(x)\n",
    "        #print(\"x after resnet\", x.size())\n",
    "        x = self.geo_net(x)\n",
    "        #print(\"x after geonet\", x.size())\n",
    "        #x = x.repeat(1, 3, 1, 1)\n",
    "        return x\n",
    "\n",
    "class geom_loss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(geom_loss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Calculate the loss for the three different outputs\n",
    "        \n",
    "        L= torch.norm( (pred[:,0,:,:] - target[:,0,:,:]), p=1) #\n",
    "        # Sum the losses (if you want to only supervise one output, you can remove the other losses from the sum)\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "csv_file = config.EXPORT3\n",
    "assert os.path.exists(csv_file)\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "train=df.sample(frac=0.8,random_state=200)\n",
    "test=df.drop(train.index)\n",
    "\n",
    "batchSize = 32\n",
    "\n",
    "# Modify the dataset to apply the transformation\n",
    "trainDataset = StevenCustomDataset(data = train)\n",
    "train_loader = DataLoader(trainDataset, batch_size=batchSize, shuffle=False)\n",
    "model = features_net().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "geoLoss = geom_loss()\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "lossMemory = []\n",
    "\n",
    "# Print the first batch\n",
    "for epoch in range(epochs):\n",
    "    for bindex, (batch) in enumerate(tqdm(train_loader, desc=\"Training batches\")):\n",
    "        images, pointImages = batch\n",
    "        #print(\"images\",images.size(), \"pointImages\",pointImages.size())\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(images)\n",
    "        #print(\"pred\", pred.size())\n",
    "        loss = geoLoss(pred, pointImages)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #print(\"loss\",loss/(size*size*batchSize))\n",
    "        lossMemory.append(loss.detach().cpu()/(size*size*len(images)))\n",
    "        if epoch % 3 == 0 and bindex == 0:\n",
    "            images = images[:,0:3,:,:]\n",
    "            fig, axs = plt.subplots(5, 4)\n",
    "            axs[0,0].imshow(images[8].permute(1, 2, 0).cpu().numpy())\n",
    "            axs[0,1].imshow(pointImages[8].permute(1, 2, 0).cpu())\n",
    "            axs[0,2].imshow(pred[8].permute(1, 2, 0).detach().cpu().numpy())\n",
    "            axs[1,0].imshow(images[11].permute(1, 2, 0).cpu().numpy())\n",
    "            axs[1,1].imshow(pointImages[11].permute(1, 2, 0).cpu())\n",
    "            axs[1,2].imshow(pred[11].permute(1, 2, 0).detach().cpu().numpy())\n",
    "            axs[2,0].imshow(images[25].permute(1, 2, 0).cpu().numpy())\n",
    "            axs[2,1].imshow(pointImages[25].permute(1, 2, 0).cpu())\n",
    "            axs[2,2].imshow(pred[25].permute(1, 2, 0).detach().cpu().numpy())\n",
    "            axs[3,0].imshow(images[12].permute(1, 2, 0).cpu().numpy())\n",
    "            axs[3,1].imshow(pointImages[12].permute(1, 2, 0).cpu())\n",
    "            axs[3,2].imshow(pred[12].permute(1, 2, 0).detach().cpu().numpy())\n",
    "            axs[4,0].imshow(images[7].permute(1, 2, 0).cpu().numpy())\n",
    "            axs[4,1].imshow(pointImages[7].permute(1, 2, 0).float().cpu())\n",
    "            axs[4,2].imshow(pred[7].permute(1, 2, 0).detach().cpu().numpy())\n",
    "            axs[4][3].plot(range(len(lossMemory)), lossMemory)\n",
    "            #print(\"max7\", torch.max(pointImages[7]))\n",
    "            plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = StevenCustomDataset(data = test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False)\n",
    "model = model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bindex, (batch) in enumerate(tqdm(test_loader, desc=\"Testing batches\")):\n",
    "        images, pointImages = batch\n",
    "        pred = model(images)\n",
    "        loss = geoLoss(pred, pointImages)\n",
    "        #print(\"loss\",loss/(size*size*batchSize))\n",
    "        pred = pred[1][0].repeat(3,1,1)\n",
    "\n",
    "        # Parameters for max pooling\n",
    "        kernel_size = 25\n",
    "        stride = 1\n",
    "        padding = (kernel_size - 1) // 2  # Set padding to ensure output size matches input size\n",
    "        \n",
    "        # Perform max pooling\n",
    "        pooled = torch.nn.functional.max_pool2d(pred, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "        # Identify locations where the pooled tensor matches the original tensor\n",
    "        local_maxima_mask = (pred == pooled) & (pred > 0)\n",
    "        local_maxima_indices = torch.nonzero(local_maxima_mask)\n",
    "        local_maxima_values = pred[local_maxima_mask]\n",
    "\n",
    "\n",
    "        #print(\"Coordinates of local maxima:\", local_maxima_indices)\n",
    "        #print(\"Values of local maxima:\", local_maxima_values)\n",
    "\n",
    "        images = images[1].permute(1, 2, 0).cpu().numpy()[:,:,0:3]  # Convert `images` to NumPy format for manipulation\n",
    "\n",
    "        for index, coordinate in enumerate(local_maxima_indices):\n",
    "            if local_maxima_values[index] > 0.2:\n",
    "                #print(coordinate)\n",
    "                # Convert `coordinate` to a NumPy array\n",
    "                coordinate_np = coordinate.cpu().numpy().astype(int)\n",
    "                \n",
    "                # Mark the local maximum on the image in red\n",
    "                images[coordinate_np[1], coordinate_np[2], 0] = 255\n",
    "        print(pred.size())\n",
    "        # Plotting images if bindex is divisible by 1 (every iteration)\n",
    "        if bindex % 1 == 0:\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(12, 10))\n",
    "            axs[0].imshow(images)\n",
    "            axs[1].imshow(pred.permute(1,2,0).detach().cpu().numpy())\n",
    "            axs[1].set_title(\"Point Image\")\n",
    "\n",
    "            # Ensure `pointImages[0]` is also converted for display\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            '''axs[1,0].imshow(images[11].permute(1, 2, 0).cpu().numpy()/255)\n",
    "            axs[1,1].imshow(pointImages[11].permute(1, 2, 0).cpu())\n",
    "            axs[1,2].imshow(pred[11].permute(1, 2, 0).detach().cpu().numpy())\n",
    "            axs[2,0].imshow(images[25].permute(1, 2, 0).cpu().numpy()/255)\n",
    "            axs[2,1].imshow(pointImages[25].permute(1, 2, 0).cpu())\n",
    "            axs[2,2].imshow(pred[25].permute(1, 2, 0).detach().cpu().numpy())\n",
    "            axs[3,0].imshow(images[12].permute(1, 2, 0).cpu().numpy()/255)\n",
    "            axs[3,1].imshow(pointImages[12].permute(1, 2, 0).cpu())\n",
    "            axs[3,2].imshow(pred[12].permute(1, 2, 0).detach().cpu().numpy())\n",
    "            axs[4,0].imshow(images[7].permute(1, 2, 0).cpu().numpy()/255)\n",
    "            axs[4,1].imshow(pointImages[7].permute(1, 2, 0).float().cpu())\n",
    "            axs[4,2].imshow(pred[7].permute(1, 2, 0).detach().cpu().numpy())\n",
    "            axs[4][3].plot(range(len(lossMemory)), lossMemory)'''\n",
    "            #plt.show()\n",
    "\n",
    "\n",
    "model = model.cpu()\n",
    "#torch.save(model.state_dict(), \"models/gaussian_points_finder.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
